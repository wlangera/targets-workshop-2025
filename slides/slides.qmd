---
title: "The {targets} pipeline tool for R"
subtitle: "Monthly presentations BMK"
date: 2025-05-13
author:
  - name:
      given: Ward
      family: Langeraert
    email: ward.langeraert@inbo.be
    orcid: 0000-0002-5900-8109
    corresponding: true
lang: en
format:
  inbo-revealjs:
    menu:
      useTextContentForMissingTitles: false
      hideMissingTitles: true
knitr:
  opts_chunk:
    echo: true
    message: false
    warning: false
editor_options: 
  chunk_output_type: console
---

## Table of contents

:::: {.columns}

::: {.column width="50%"}
- Introduction
  - What is **targets**?
  - Getting started
  - Tracking changes
- Advanced usage
  - Branching
  - Project management
  - Much more ...

:::

::: {.column width="50%"}
- Exercises
  - Challenge 1
  - Challenge 2
  - INTERMEZZO - Beyond **targets**
  - Challenge 3 - BONUS
  - Solutions
  
:::

::::

# Introduction
## What is **targets**?

- **Pipeline tool** for statistics and data science in R
  - Coordinates complex, computationally intensive **R workflows**  
  - Skips steps that are already up-to-date, saving time and resources
- Ensures results stay **trustworthy** and **reproducible**

**targets** manual:\
<https://books.ropensci.org/targets/>

## Getting started

1. **Write R functions** for your analysis and save them in the `R/` folder  
2. Use `use_targets()` to scaffold your project and **create the `_targets.R` file**
3. **Edit `_targets.R`** to define your pipeline (guided by helpful comments)  
4. **Visualise the pipeline** with `tar_visnetwork()`  
5. **Run the pipeline** with `tar_make()`  
6. **Access results** using `tar_read()` and other utility functions  

---

### 1. Write R functions

```r
# Functions for example pipeline
get_data <- function(file) {
  data_set <- read_csv(file, col_types = cols())
  data_set[complete.cases(data_set), ]
}

fit_model <- function(data) {
  lm(Ozone ~ Temp, data) |>
    coefficients()
}

plot_model <- function(model, data) {
  ggplot(data) +
    geom_point(aes(x = Temp, y = Ozone)) +
    geom_abline(intercept = model[1], slope = model[2])
}
```

---

- `airquality` example dataset

```{r, echo=FALSE}
dir.create(here::here("data"), showWarnings = FALSE)
```

```r
readr::write_csv(
  airquality,
  here::here("data", "example_data.csv")
)
```

\

```{r, echo=TRUE}
head(airquality, n = 4)
```

---

### 2. Create `_targets.R` pipeline file

- Load **targets** package.

```{r}
library(targets)
```

- `use_targets()`

```r
use_targets(
  script = here::here("source", "pipeline_example", "_targets.R")
)
```

---

- `_targets.R` pipeline file

```r
# Created by use_targets().
# Follow the comments below to fill in this target script.
# Then follow the manual to check and run the pipeline:
#   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline

# Load packages required to define the pipeline:
library(targets)
# library(tarchetypes) # Load other packages as needed.

# Set target options:
tar_option_set(
  packages = c("tibble") # Packages that your targets need for their tasks.
  # format = "qs", # Optionally set the default storage format. qs is fast.
  #
  # Pipelines that take a long time to run may benefit from
  # optional distributed computing. To use this capability
  # in tar_make(), supply a {crew} controller
  # as discussed at https://books.ropensci.org/targets/crew.html.
  # Choose a controller that suits your needs. For example, the following
  # sets a controller that scales up to a maximum of two workers
  # which run as local R processes. Each worker launches when there is work
  # to do and exits if 60 seconds pass with no tasks to run.
  #
  #   controller = crew::crew_controller_local(workers = 2, seconds_idle = 60)
  #
  # Alternatively, if you want workers to run on a high-performance computing
  # cluster, select a controller from the {crew.cluster} package.
  # For the cloud, see plugin packages like {crew.aws.batch}.
  # The following example is a controller for Sun Grid Engine (SGE).
  # 
  #   controller = crew.cluster::crew_controller_sge(
  #     # Number of workers that the pipeline can scale up to:
  #     workers = 10,
  #     # It is recommended to set an idle time so workers can shut themselves
  #     # down if they are not running tasks.
  #     seconds_idle = 120,
  #     # Many clusters install R as an environment module, and you can load it
  #     # with the script_lines argument. To select a specific verison of R,
  #     # you may need to include a version string, e.g. "module load R/4.3.2".
  #     # Check with your system administrator if you are unsure.
  #     script_lines = "module load R"
  #   )
  #
  # Set other options as needed.
)

# Run the R scripts in the R/ folder with your custom functions:
tar_source()
# tar_source("other_functions.R") # Source other scripts as needed.

# Replace the target list below with your own:
list(
  tar_target(
    name = data,
    command = tibble(x = rnorm(100), y = rnorm(100))
    # format = "qs" # Efficient storage for general data objects.
  ),
  tar_target(
    name = model,
    command = coefficients(lm(y ~ x, data = data))
  )
)
```

---

### 3. Edit `_targets.R` pipeline file

- Lets clean up this file to the basics

```r
# Load packages required to define the pipeline:
library(targets)
library(here)

# Set target options:
tar_option_set(packages = c("readr", "ggplot2"))

# Load custom functions and small input objects into the R session
source(here("source", "R", "example_functions.R"))
data_path <- here("data")

# Write the pipeline, a list of target objects
list(
  tar_target(file, file.path(data_path, "example_data.csv"), format = "file"),
  tar_target(data, get_data(file)),
  tar_target(model, fit_model(data)),
  tar_target(plot, plot_model(model, data))
)
```

---

### 4. Visualise the pipeline

- `tar_visnetwork()`

```{r}
tar_visnetwork(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

---

### 5. Run the pipeline

- `tar_make()`

```{r}
tar_make(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

---

```{r}
tar_visnetwork(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

---

- The pipeline is up to date, rerunning the pipeline is very fast

```{r}
tar_make(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

---

### 6. Access results

- `tar_read()`

```{r}
model_coefs <- tar_read(
  "model",
  store = here::here("source", "pipeline_example", "_targets")
)

model_coefs
```

---

```{r}
tar_read(
  "plot",
  store = here::here("source", "pipeline_example", "_targets")
)
```

---

## Tracking changes

- The **targets** package notices when you make changes to code and data
  - Change code
    - E.g. increase point size in plot function

```r
plot_model <- function(model, data) {
  ggplot(data) +
    geom_point(aes(x = Temp, y = Ozone), size = 3) + # Add size 3
    geom_abline(intercept = model[1], slope = model[2])
}
```

---

```{r, echo=FALSE}
# Read the file
lines <- readLines(here::here("source", "R", "example_functions.R"))

# Find and modify the line with geom_point
lines <- gsub(
  "geom_point\\(aes\\(x = Temp, y = Ozone\\)\\)",
  "geom_point(aes(x = Temp, y = Ozone), size = 3)",
  lines
)

# Write the updated file
writeLines(lines, here::here("source", "R", "example_functions.R"))
```

```{r}
tar_visnetwork(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

```{r, echo=FALSE}
# Read the file
lines <- readLines(here::here("source", "R", "example_functions.R"))

# Find and modify the line with geom_point
lines <- gsub(
  "geom_point\\(aes\\(x = Temp, y = Ozone\\), size = 3\\)",
  "geom_point(aes(x = Temp, y = Ozone))",
  lines
)

# Write the updated file
writeLines(lines, here::here("source", "R", "example_functions.R"))
```

```{r, echo=FALSE, results='hide'}
tar_make(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

## Tracking changes

- The **targets** package notices when you make changes to code and data
  - Change code
  - Change data
    - `format = "file"` in `tar_target()`
    - E.g. select first 100 rows of `airquality` dataset
  
```{r}
readr::write_csv(
  head(airquality, n = 100),
  here::here("data", "example_data.csv")
)
```

---

```{r}
tar_visnetwork(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

```{r, echo=FALSE}
readr::write_csv(
  airquality,
  here::here("data", "example_data.csv")
)
```

```{r, echo=FALSE, results='hide'}
tar_make(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets")
)
```

# Advanced usage
## Branching

- A single target create multiple sub-targets (**branches**)
  - Parallel execution of branches
  - Efficient re-use: only updated branches re-run
  - Scales elegantly with large or complex inputs
- Static vs Dynamic Branching
  - **Dynamic**: Target creates sub-targets at runtime based on upstream results
  - **Static**: Target depends on a fixed list — created ahead of time

---

### Dynamic branching

- **targets** offers multiple **branching patterns**

**1.** `map()` – One branch per tuple of elements:

- Useful for one-to-one processing

```r
tar_target(a, c(1, 2))
tar_target(b, c("x", "y"))
tar_target(result, paste0("result", a, b), pattern = map(a, b))
```

➡ Creates: `result1x`, `result2y`

---

**2.** `cross()` – All combinations of elements:

- Creates a branch for every combination of upstream inputs

```r
tar_target(a, c(1, 2))
tar_target(b, c("x", "y"))
tar_target(result, paste0("result", a, b), pattern = cross(a, b))
```

➡ Creates: `result1x`, `result1y` `result2x`, `result2y`

---

**3.** `slice()` – Specific slices/branches only:

- Selects certain branches from an upstream branched target.

```r
tar_target(data, 1:5)
tar_target(square, data^2, pattern = map(data))
tar_target(selected, square, pattern = slice(square, index = c(2, 4)))
```

➡ Only runs branches 2 and 4 of `square`

---

**4.** `head()`/`tail()`/`sample()` – Selection of elements:

- `head()` processes only the first $N$ elements

```r
tar_target(data, 1:10)
tar_target(head_values, data, pattern = head(data, n = 3))
```

➡ Branches over values 1, 2, 3


- `tail()` processes the last $N$ elements and `sample()` over a random sample of $N$ elements

---

### Static branching

- Define group of targets **before** the pipeline runs
  - Generate targets up front (not during execution)
  - Does **not** depend on runtime data
- Best suited for a **small number of heterogeneous** targets
  - E.g. iterating over different datasets and analysis methods

## Project management

1. Use nested R projects
   - pro: clean, relative paths
   - con: need to know when to open which `.Rproj`
2. Use **here** package
   - pro: no need for separate `.Rproj` files or configuration scripts
   - con: paths needed every time to run/handle each pipeline

---

3. Use `tar_config_set()`
   - pro: no need for separate `.Rproj` files or file paths
   - con: separate config. script and use `Sys.setenv()` each time

```r
tar_config_set(script = "script_a.R", store = "store_a", project = "project_a")
tar_config_set(script = "script_b.R", store = "store_b", project = "project_b")
```

- Set environment variables and run/read pipeline

```r
Sys.setenv(TAR_PROJECT = "project_a")
tar_make()
tar_read(target_abc)
Sys.setenv(TAR_PROJECT = "project_b")
tar_make()
tar_read(target_123)
```

## Much more ...

**targets** manual:\
<https://books.ropensci.org/targets/>

```{r, echo=FALSE, results='hide'}
# Remove targets store
tar_destroy(
  script = here::here("source", "pipeline_example", "_targets.R"),
  store = here::here("source", "pipeline_example", "_targets"),
  ask = FALSE
)
```

# Exercises
## Challenge 1

- Create a **targets** pipeline
  1. Read example datasets from the **b3data** data package ([link](https://github.com/b-cubed-eu/b3data-scripts))
     - `bird_cube_belgium_mgrs10` (tabular) and `mgrs10_refgrid_belgium` (polygon grid)
  2. Filter out data with minimal coord. uncertainty > 10 km
  3. Calculate species richness
  4. Visualise richness per grid cell (join with reference grid)
- Create an R Markdown report (outside pipeline) that visualises the plot

---

Tips and tricks:

- Use **frictionless** and **sf** to read the data, see [this README](https://github.com/b-cubed-eu/b3data-scripts) on how to get the data
- Use **dplyr** for data wrangling and joining of datasets
- Use **sf** to restore the CRS after joining
- Use **ggplot2** for visualisation


- Store the grid in a list to work with sf objects: `return(list(spatial_resource))`

```r
plot_richness(
  richness_cube = bird_richness_df,
  ref_grid = mgrs10_refgrid[[1]]
)
```

## Challenge 2

- Build further on your pipeline from *Challenge 1*
  - Filter the data
    - 2020
    - Top 20 species that occur in most grid cells
  - Use dynamic branching to create presence-absence plots for each species separately
- Add the plots in your external R Markdown report

## INTERMEZZO - Beyond **targets**
### R Targetopia

- <https://wlandau.github.io/targetopia/>

---

### **tarchetypes** package

- A companion package with **prebuilt target and pipeline archetypes**
- Simplifies complex workflows into **concise, readable syntax**
- Enhances **reproducibility** by standardizing common patterns

---

- Some examples
  - `tarchetypes::tar_file()`
    - Shorthand for `format = "file"` in `targets::tar_target()`
  - `tarchetypes::tar_group_by()`
    - Combines `dplyr::group_by()` and `targets::tar_group()`
  - `tarchetypes::tar_render()`
    - Render R Markdown reports in the pipeline

## Challenge 3 - BONUS

- Use **tarchetypes** package for *Challenge 2*
  - To group the species in dynamic branching
  - To create the R Markdown report at the end of of the pipeline

## Solutions

:::: {.columns}

::: {.column width="50%"}
- Provided in separate branches
  - `challenge-1-solution`
  - `challenge-2-solution`
  - `challenge-3-solution`

:::

::: {.column width="50%"}
- R Markdown with solutions
  - `source/solutions_ward.Rmd`
    - runs pipeline `bird_analysis_pipeline`
    - shows output figures
  
:::

::::
